{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Toki-Pona datasets\n",
    "\n",
    "This notebook consolidates the data in each of the different folders into a single dataset. It generates a file for sentence translations between Toki Pona, English, and optionally Chinese, a file for sentences in Toki Pona, and file containing entire documents in each language (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_TYPES = [\n",
    "    ENCYCLOPEDIA_ARTICLE := 'encyclopedia article',\n",
    "    ARTICLE := 'article',\n",
    "    BLOG_POST := 'blog post',\n",
    "    MAGAZINE := 'magazine',\n",
    "    BIBLE := 'biblical text',\n",
    "    STORY := 'story',\n",
    "    POEM := 'poem',\n",
    "    SCREENPLAY := 'screenplay',\n",
    "    BOOK := 'book',\n",
    "    CHAPTER := 'chapter',\n",
    "    ESSAY := 'essay',\n",
    "    CHAT := 'chat',\n",
    "    OTHER := 'other',\n",
    "]\n",
    "\n",
    "FORMATS = [\n",
    "    TEXT := 'text',\n",
    "    MARKDOWN := 'markdown',\n",
    "    IRC_LOG := 'irc log',\n",
    "]\n",
    "\n",
    "sentence_translations = pd.DataFrame(columns=['id', 'tok', 'eng', 'cmn'])\n",
    "sentences = pd.DataFrame(columns=['id', 'content_type', 'sentence'])\n",
    "documents = pd.DataFrame(columns=['id', 'name', 'content_type', 'tok', 'eng', 'cmn'])\n",
    "chapters = pd.DataFrame(columns=['id', 'name', 'chapter_number', 'content_type', 'tok', 'eng', 'cmn'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence translations\n",
    "\n",
    "Go through the files in the `phrases` folder and generate a file containing the sentence translations. These files are:\n",
    "\n",
    "|File|Language|Description|Length|\n",
    "|----|--------|-----------|------|\n",
    "|`common.md`|Toki Pona and English|Common phrases and responses|~100 pairs|\n",
    "|`common2.tsv`|Toki Pona and English|Common sentences|~2000 pairs|\n",
    "|`tatoeba-dev.eng-toki.tsv`|Toki Pona and English|Some Tatoeba translations between Toki Pona and English ([from this dataset dated to 2021](https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt/blob/main/dev/tatoeba-dev.eng-toki.tsv))|~8000 pairs|\n",
    "|`tatoeba-test.eng-toki.tsv`|Toki Pona and English|Some Tatoeba translations between Toki Pona and English ([from this dataset dated to 2021](https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt/blob/main/test/tatoeba-test.eng-toki.tsv))|~5000 pairs|\n",
    "|`translations.tsv`|Toki Pona, English, and Chinese|Tatoeba translations between Toki Pona, English, and Chinese (dated 4/14/2023)|~33000 pairs|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1907 1907\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/common2.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for tok, eng in tsv:\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, None]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10609 10609\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/tatoeba-dev.eng-toki.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for _, _, eng, tok in tsv:\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, None]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15599 15599\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/tatoeba-test.eng-toki.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for _, _, eng, tok in tsv:\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, None]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48740 48740\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/translations.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for row in tsv:\n",
    "    if len(row) == 4:\n",
    "        _, tok, eng, cmn = row\n",
    "    elif len(row) == 3:\n",
    "        _, tok, eng = row\n",
    "        cmn = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    if cmn == '':\n",
    "        cmn = None\n",
    "\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, cmn]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toki Pona: 402085\n",
      "Toki Pona: 402085 English: 254765 Chinese: 5761.0\n"
     ]
    }
   ],
   "source": [
    "print('Toki Pona:', sentences['sentence'].str.split().str.len().sum())\n",
    "print('Toki Pona:', int(sentence_translations['tok'].str.split().str.len().sum()), 'English:', int(sentence_translations['eng'].str.split().str.len().sum()), 'Chinese:', sentence_translations['cmn'].str.split().str.len().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences and translations to a file.\n",
    "sentences.to_csv(os.path.expanduser(\"phrases/sentences.tsv\"), sep='\\t', index=False)\n",
    "sentence_translations.to_csv(os.path.expanduser(\"phrases/sentence_translations.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "# Reload them and confirm that they are the same.\n",
    "sentences_copy = pd.read_csv(os.path.expanduser(\"phrases/sentences.tsv\"), sep='\\t')\n",
    "assert sentences.equals(sentences_copy)\n",
    "\n",
    "sentence_translations_copy = pd.read_csv(os.path.expanduser(\"phrases/sentence_translations.tsv\"), sep='\\t')\n",
    "assert sentence_translations.equals(sentence_translations_copy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents and translations\n",
    "\n",
    "Go through the files in each of the folders and add their entire contents to each field in the dataset. These files are in:\n",
    "\n",
    "|Folder|Language|Description|Length|\n",
    "|------|--------|-----------|------|\n",
    "|`articles`|Toki Pona and English|Articles from Lipu Kule|Unknown|\n",
    "|`chat`|Toki Pona and English|Chat logs from Unknown|Unknown|\n",
    "|`comments`|Toki Pona|Comments on blog posts and reviews of books|Unknown|\n",
    "|`dictionary`|Toki Pona and English|Toki Pona dictionary|Unknown|\n",
    "|`encyclopedia`|Toki Pona|Articles from Wikipesija. The name of the document is the subject of the article.|Unknown|\n",
    "|`magazines`|Toki Pona|Entire copies of Lipu Tenpo|Unknown|\n",
    "|`stories`|Toki Pona and English|Stories in Toki Pona and English.|Unknown|\n",
    "|`poems`|Toki Pona|Poems in Toki Pona.|Unknown|\n",
    "|`screenplays`|Toki Pona and English|Screenplays and their translations.|Unknown|\n",
    "|`bible`|Toki Pona and English|Texts relating to the bible.|Unknown|\n",
    "|`livejournal-blog`|Toki Pona and English|Texts from LiveJournal blogs.|Unknown|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame(columns=['id', 'name', 'content_type', 'tok', 'eng', 'cmn'])\n",
    "\n",
    "def get_files(dir, ext):\n",
    "    # Get all the files in articles/tok/ and articles/eng/\n",
    "    tok_files = glob(os.path.expanduser(f\"{dir}/tok/*.{ext}\"))\n",
    "    eng_files = glob(os.path.expanduser(f\"{dir}/eng/*.{ext}\"))\n",
    "\n",
    "    # Strip the path and extension from the filenames\n",
    "    tok_files = [os.path.basename(f) for f in tok_files]\n",
    "    eng_files = [os.path.basename(f) for f in eng_files]\n",
    "\n",
    "    # Get the shared set of files\n",
    "    tok_files = set(tok_files)\n",
    "    eng_files = set(eng_files)\n",
    "    shared_files = tok_files.intersection(eng_files)\n",
    "\n",
    "    # Get the set of files that are only in tok/ or eng/\n",
    "    tok_only_files = tok_files.difference(eng_files)\n",
    "    eng_only_files = eng_files.difference(tok_files)\n",
    "\n",
    "    return shared_files, tok_only_files, eng_only_files\n",
    "\n",
    "def read_files(dir, content_type):\n",
    "    shared_files, tok_only_files, _ = get_files(dir, \"*\")\n",
    "\n",
    "    # Get the shared files and save them in the documents table\n",
    "    for f in shared_files:\n",
    "        tok = open(os.path.expanduser(f\"{dir}/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "        eng = open(os.path.expanduser(f\"{dir}/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "        if tok == '':\n",
    "            tok = None\n",
    "        if eng == '':\n",
    "            eng = None\n",
    "        documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), content_type, tok, eng, None]\n",
    "\n",
    "    # Get the files that are only in tok/ and save them in the documents table\n",
    "    for f in tok_only_files:\n",
    "        tok = open(os.path.expanduser(f\"{dir}/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "        if tok == '':\n",
    "            tok = None\n",
    "        documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), content_type, tok, None, None]\n",
    "\n",
    "    print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "75\n",
      "116\n",
      "140\n",
      "141\n",
      "500\n",
      "1840\n",
      "1928\n",
      "1946\n",
      "2028\n",
      "2029\n",
      "2091\n"
     ]
    }
   ],
   "source": [
    "read_files(\"articles\", ARTICLE)\n",
    "read_files(\"bible\", BIBLE)\n",
    "read_files(\"chat\", CHAT)\n",
    "read_files(\"comments\", CHAT)\n",
    "read_files(\"dictionary\", OTHER)\n",
    "read_files(\"encyclopedia\", ENCYCLOPEDIA_ARTICLE)\n",
    "read_files(\"jan Kipu Corpus\", OTHER)\n",
    "read_files(\"livejournal-blog\", BLOG_POST)\n",
    "read_files(\"magazines\", MAGAZINE)\n",
    "read_files(\"poems\", POEM)\n",
    "read_files(\"screenplays\", SCREENPLAY)\n",
    "read_files(\"stories\", STORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toki Pona: 1167019 English: 63119 Chinese: 0\n"
     ]
    }
   ],
   "source": [
    "# For each document, find the word count and add it all up\n",
    "print('Toki Pona:', int(documents['tok'].str.split().str.len().sum()), 'English:', int(documents['eng'].str.split().str.len().sum()), 'Chinese:', documents['cmn'].str.split().str.len().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences and translations to a file.\n",
    "documents.to_csv(os.path.expanduser(\"output-documents.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "# Reload them and confirm that they are the same, convert NaNs to None\n",
    "documents_copy = pd.read_csv(os.path.expanduser(\"output-documents.tsv\"), sep='\\t')\n",
    "documents_copy = documents_copy.replace(np.nan, None)\n",
    "\n",
    "assert documents.equals(documents_copy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapters and Translations\n",
    "\n",
    "Go through the files of books and screenplays and add their corresponding translated chapters to this dataset. These files are:\n",
    "\n",
    "|Folder|Language|Description|Length|\n",
    "|------|--------|-----------|------|\n",
    "|`bible`|Toki Pona and English|The Gospel of John|22 chapters|\n",
    "|`screenplays`|Toki Pona and English|Monty Python and the Holy Grail|24 scenes|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = pd.DataFrame(columns=['id', 'name', 'chapter_number', 'content_type', 'tok', 'eng', 'cmn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get the chapters for the gospel of john\n",
    "'''\n",
    "\n",
    "tok_gospel_of_john = open(os.path.expanduser(f\"bible/tok/gospel of john.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "tok_gospel_of_john_chapters = re.split('\\nkipisi[^\\.\\n]+\\n', tok_gospel_of_john)\n",
    "\n",
    "eng_gospel_of_john = open(os.path.expanduser(f\"bible/eng/gospel of john.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "eng_gospel_of_john_chapters = list(filter(lambda x: x != '', map(lambda x: x.strip(), re.split('Chapter[^\\.\\n]+\\n', eng_gospel_of_john))))\n",
    "\n",
    "for i, (tok, eng) in enumerate(zip(tok_gospel_of_john_chapters, eng_gospel_of_john_chapters)):\n",
    "    chapters.loc[len(chapters)] = [len(chapters), 'gospel of john', i + 1, BIBLE, tok, eng, None]\n",
    "\n",
    "print(len(tok_gospel_of_john_chapters), len(eng_gospel_of_john_chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get the chapters for the monty python screenplay\n",
    "'''\n",
    "\n",
    "tok_monty_python = open(os.path.expanduser(f\"screenplays/tok/monty python.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "tok_monty_python_chapters = re.split('\\nkipisi[^\\.\\n]+\\n', tok_monty_python)\n",
    "\n",
    "eng_monty_python = open(os.path.expanduser(f\"screenplays/eng/monty python.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "eng_monty_python_chapters = list(filter(lambda x: x != '', map(lambda x: x.strip(), re.split('Scene \\d+\\n', eng_monty_python))))\n",
    "\n",
    "for i, (tok, eng) in enumerate(zip(tok_monty_python_chapters, eng_monty_python_chapters)):\n",
    "    chapters.loc[len(chapters)] = [len(chapters), 'Monty Python and the Holy Grail', i + 1, SCREENPLAY, tok, eng, None]\n",
    "\n",
    "print(len(tok_monty_python_chapters), len(eng_monty_python_chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toki Pona: 34077 English: 29438 Chinese: 0\n"
     ]
    }
   ],
   "source": [
    "# For each document, find the word count and add it all up\n",
    "print('Toki Pona:', int(chapters['tok'].str.split().str.len().sum()), 'English:', int(chapters['eng'].str.split().str.len().sum()), 'Chinese:', chapters['cmn'].str.split().str.len().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences and translations to a file.\n",
    "chapters.to_csv(os.path.expanduser(\"chapters.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "# Reload them and confirm that they are the same, convert NaNs to None\n",
    "chapters_copy = pd.read_csv(os.path.expanduser(\"chapters.tsv\"), sep='\\t')\n",
    "chapters_copy = chapters.replace(np.nan, None)\n",
    "\n",
    "assert chapters.equals(chapters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
