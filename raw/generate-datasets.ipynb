{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Toki-Pona datasets\n",
    "\n",
    "This notebook consolidates the data in each of the different folders into a single dataset. It generates a file for sentence translations between Toki Pona, English, and optionally Chinese, a file for sentences in Toki Pona, and file containing entire documents in each language (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_TYPES = [\n",
    "    ENCYCLOPEDIA_ARTICLE := 'encyclopedia article',\n",
    "    ARTICLE := 'article',\n",
    "    BLOG_POST := 'blog post',\n",
    "    MAGAZINE := 'magazine',\n",
    "    BIBLE := 'biblical text',\n",
    "    STORY := 'story',\n",
    "    POEM := 'poem',\n",
    "    SCREENPLAY := 'screenplay',\n",
    "    BOOK := 'book',\n",
    "    CHAPTER := 'chapter',\n",
    "    ESSAY := 'essay',\n",
    "    CHAT := 'chat',\n",
    "    OTHER := 'other',\n",
    "]\n",
    "\n",
    "FORMATS = [\n",
    "    TEXT := 'text',\n",
    "    MARKDOWN := 'markdown',\n",
    "    IRC_LOG := 'irc log',\n",
    "]\n",
    "\n",
    "sentence_translations = pd.DataFrame(columns=['id', 'tok', 'eng', 'cmn'])\n",
    "sentences = pd.DataFrame(columns=['id', 'content_type', 'sentence'])\n",
    "documents = pd.DataFrame(columns=['id', 'name', 'content_type', 'tok', 'eng', 'cmn'])\n",
    "chapters = pd.DataFrame(columns=['id', 'name', 'chapter_number', 'content_type', 'tok', 'eng', 'cmn'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence translations\n",
    "\n",
    "Go through the files in the `phrases` folder and generate a file containing the sentence translations. These files are:\n",
    "\n",
    "|File|Language|Description|Length|\n",
    "|----|--------|-----------|------|\n",
    "|`common.md`|Toki Pona and English|Common phrases and responses|~100 pairs|\n",
    "|`common2.tsv`|Toki Pona and English|Common sentences|~2000 pairs|\n",
    "|`tatoeba-dev.eng-toki.tsv`|Toki Pona and English|Some Tatoeba translations between Toki Pona and English ([from this dataset dated to 2021](https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt/blob/main/dev/tatoeba-dev.eng-toki.tsv))|~8000 pairs|\n",
    "|`tatoeba-test.eng-toki.tsv`|Toki Pona and English|Some Tatoeba translations between Toki Pona and English ([from this dataset dated to 2021](https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt/blob/main/test/tatoeba-test.eng-toki.tsv))|~5000 pairs|\n",
    "|`translations.tsv`|Toki Pona, English, and Chinese|Tatoeba translations between Toki Pona, English, and Chinese (dated 4/14/2023)|~33000 pairs|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1907 1907\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/common2.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for tok, eng in tsv:\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, None]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10609 10609\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/tatoeba-dev.eng-toki.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for _, _, eng, tok in tsv:\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, None]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15599 15599\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/tatoeba-test.eng-toki.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for _, _, eng, tok in tsv:\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, None]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48740 48740\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.expanduser(\"phrases/translations.tsv\"), \"r\", encoding=\"utf-8\")\n",
    "tsv = [line.strip().split(\"\\t\") for line in f]\n",
    "f.close()\n",
    "for row in tsv:\n",
    "    if len(row) == 4:\n",
    "        _, tok, eng, cmn = row\n",
    "    elif len(row) == 3:\n",
    "        _, tok, eng = row\n",
    "        cmn = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    if cmn == '':\n",
    "        cmn = None\n",
    "\n",
    "    sentences.loc[len(sentences)] = [len(sentences), OTHER, tok]\n",
    "    sentence_translations.loc[len(sentence_translations)] = [len(sentence_translations), tok, eng, cmn]\n",
    "\n",
    "print(len(sentences), len(sentence_translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toki Pona: 402085\n",
      "Toki Pona: 402085 English: 254765 Chinese: 5761.0\n"
     ]
    }
   ],
   "source": [
    "print('Toki Pona:', sentences['sentence'].str.split().str.len().sum())\n",
    "print('Toki Pona:', int(sentence_translations['tok'].str.split().str.len().sum()), 'English:', int(sentence_translations['eng'].str.split().str.len().sum()), 'Chinese:', sentence_translations['cmn'].str.split().str.len().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences and translations to a file.\n",
    "sentences.to_csv(os.path.expanduser(\"phrases/sentences.tsv\"), sep='\\t', index=False)\n",
    "sentence_translations.to_csv(os.path.expanduser(\"phrases/sentence_translations.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "# Reload them and confirm that they are the same.\n",
    "sentences_copy = pd.read_csv(os.path.expanduser(\"phrases/sentences.tsv\"), sep='\\t')\n",
    "assert sentences.equals(sentences_copy)\n",
    "\n",
    "sentence_translations_copy = pd.read_csv(os.path.expanduser(\"phrases/sentence_translations.tsv\"), sep='\\t')\n",
    "assert sentence_translations.equals(sentence_translations_copy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents and translations\n",
    "\n",
    "Go through the files in each of the folders and add their entire contents to each field in the dataset. These files are in:\n",
    "\n",
    "|Folder|Language|Description|Length|\n",
    "|------|--------|-----------|------|\n",
    "|`articles`|Toki Pona and English|Articles from Lipu Kule|Unknown|\n",
    "|`chat`|Toki Pona and English|Chat logs from Unknown|Unknown|\n",
    "|`comments`|Toki Pona|Comments on blog posts and reviews of books|Unknown|\n",
    "|`dictionary`|Toki Pona and English|Toki Pona dictionary|Unknown|\n",
    "|`encyclopedia`|Toki Pona|Articles from Wikipesija. The name of the document is the subject of the article.|Unknown|\n",
    "|`magazines`|Toki Pona|Entire copies of Lipu Tenpo|Unknown|\n",
    "|`stories`|Toki Pona and English|Stories in Toki Pona and English.|Unknown|\n",
    "|`poems`|Toki Pona|Poems in Toki Pona.|Unknown|\n",
    "|`screenplays`|Toki Pona and English|Screenplays and their translations.|Unknown|\n",
    "|`bible`|Toki Pona and English|Texts relating to the bible.|Unknown|\n",
    "|`livejournal-blog`|Toki Pona and English|Texts from LiveJournal blogs.|Unknown|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame(columns=['id', 'name', 'content_type', 'tok', 'eng', 'cmn'])\n",
    "def get_files(dir, ext):\n",
    "    # Get all the files in articles/tok/ and articles/eng/\n",
    "    tok_files = glob(os.path.expanduser(f\"{dir}/tok/*.{ext}\"))\n",
    "    eng_files = glob(os.path.expanduser(f\"{dir}/eng/*.{ext}\"))\n",
    "\n",
    "    # Strip the path and extension from the filenames\n",
    "    tok_files = [os.path.basename(f) for f in tok_files]\n",
    "    eng_files = [os.path.basename(f) for f in eng_files]\n",
    "\n",
    "    # Get the shared set of files\n",
    "    tok_files = set(tok_files)\n",
    "    eng_files = set(eng_files)\n",
    "    shared_files = tok_files.intersection(eng_files)\n",
    "\n",
    "    # Get the set of files that are only in tok/ or eng/\n",
    "    tok_only_files = tok_files.difference(eng_files)\n",
    "    eng_only_files = eng_files.difference(tok_files)\n",
    "\n",
    "    return shared_files, tok_only_files, eng_only_files\n",
    "\n",
    "shared_files, tok_only_files, eng_only_files = get_files(\"articles\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"articles/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"articles/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), ARTICLE, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"articles/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), ARTICLE, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"stories\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"stories/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"stories/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), STORY, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"stories/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), STORY, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"bible\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"bible/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"bible/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), BIBLE, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"bible/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), BIBLE, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"bible\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"bible/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"bible/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), BLOG_POST, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"bible/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), BLOG_POST, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"comments\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"comments/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"comments/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), CHAT, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"comments/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), CHAT, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"jan Kipu Corpus\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"jan Kipu Corpus/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"jan Kipu Corpus/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), OTHER, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"jan Kipu Corpus/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), OTHER, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1542\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"magazines\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"magazines/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"magazines/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), MAGAZINE, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"magazines/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), MAGAZINE, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"poems\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"poems/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"poems/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), POEM, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"poems/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), POEM, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"screenplays\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"screenplays/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"screenplays/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), SCREENPLAY, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"screenplays/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), SCREENPLAY, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"encyclopedia\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"encyclopedia/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"encyclopedia/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), ENCYCLOPEDIA_ARTICLE, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"encyclopedia/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), ENCYCLOPEDIA_ARTICLE, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n"
     ]
    }
   ],
   "source": [
    "shared_files, tok_only_files, eng_only_files = get_files(\"chat\", \"*\")\n",
    "\n",
    "# Get the shared files and save them in the documents table\n",
    "for f in shared_files:\n",
    "    tok = open(os.path.expanduser(f\"chat/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    eng = open(os.path.expanduser(f\"chat/eng/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    if eng == '':\n",
    "        eng = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), CHAT, tok, eng, None]\n",
    "\n",
    "# Get the files that are only in tok/ and save them in the documents table\n",
    "for f in tok_only_files:\n",
    "    tok = open(os.path.expanduser(f\"chat/tok/{f}\"), \"r\", encoding=\"utf-8\").read()\n",
    "    if tok == '':\n",
    "        tok = None\n",
    "    documents.loc[len(documents)] = [len(documents), os.path.basename(f).replace('__', '_').replace('__', '_').replace('_', ' '), CHAT, tok, None, None]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible.txt\n",
      "lipu nanpa wan pi jan Mose.\n",
      "\n",
      "tenpo wan la jan sewi Jawe li pali e sewi laso en ma suli. taso, ma suli li jo e ala. ale li pimeja.\n",
      "jan kon pi jan sewi Jawe li sewi pi telo suli.\n",
      "jan sewi Jawe li toki e ni: 'o suno li wile'. suno li kama.\n",
      "jan sewi Jawe li lukin e suno li pilin: suno li pona. jan sewi Jawe li pana e suno sama ala pi kon pimeja.\n",
      "jan sewi Jawe li pana e nimi pi suno: tenpo suno.\n",
      "jan sewi Jawe li pana e nimi pi kon pimeja: tenpo pimeja.\n",
      "tenpo ni li tenpo suno nanpa wan.\n",
      "\n",
      " \n",
      "\n",
      "jan sewi Jawe li toki e ni: mi pana e sike sewi laso pi telo. ona li pali e ni: telo wan li sama ala pi telo tu.\n",
      "jan sewi Jawe li pali e ni. sike sewi laso li lon.\n",
      " telo sewi pi sike sewi laso li sama ala pi telo anpa pi sike sewi laso.\n",
      "jan sewi Jawe li pana e nimi  pi sike sewi laso. tenpo ni li tenpo suno nanpa tu.\n",
      "\n",
      " \n",
      "\n",
      "tenpo sin la jan sewi Jawe li toki e ni: o telo ale li kama insa telo wan. o ma kiwen li lon. ni li lon.\n",
      "jan sewi Jawe li nimi e ma en telo suli. jan sewi li lukin. ale li pona.\n",
      "\n",
      " \n",
      "\n",
      "tenpo sin la jan sewi Jawe li toki e ni: ma li pana e kasi mute. \n",
      "kasi li pana e kiwen unpa li pana e kasi sin. jan sewi li lukin. ale li pona.\n",
      "tenpo ni li tenpo suno nanpa wan tu.\n",
      "\n",
      " \n",
      "\n",
      "tenpo sin la jan sewi Jawe li toki e ni: o kama suno lili mute insa sewi laso. ona li pali e ni: tenpo suno \n",
      "en tenpo pimeja li sama ala. ona li nimi pi tenpo. ona li seli.\n",
      "sike suli tu li lon insa sewi laso li suno li mun. suno en mun li pana suno tawa ma. jan sewi li lukin. ale li pona.\n",
      "tenpo ni li tenpo suno nanpa tu tu.\n",
      "\n",
      " \n",
      "\n",
      "tenpo sin la jan sewi Jawe li toki e ni: o kala mute li tawa telo. o waso mute li tawa kon.\n",
      "jan sewi Jawe li pali e soweli kala suli mute en kala mute en waso mute.  jan sewi li lukin. ale li pona.\n",
      "tenpo ni li tenpo suno nanpa luka.\n",
      "\n",
      " \n",
      "\n",
      "tenpo sin la jan sewi Jawe li toki e ni: o ma li pana e soweli mute. ni li lon. \n",
      "jan sewi Jawe li pali e soweli mute. jan sewi li lukin. ale li pona.\n",
      "\n",
      " \n",
      "\n",
      "tenpo sin la jan sewi Jawe li toki e ni: mi pali e jan. jan li lukin sama e mi. ona li jan lawa pi\n",
      "kala ale en waso ale en soweli ale en ma ale en soweli tawa ma ale.\n",
      "jan sewi Jawe li pali e jan mije en jan meli. jan sewi Jawe li pana e pona tawa jan mije en jan meli.\n",
      "ona li toki e ni: o sina pale e jan sin. o sina kama insa ma ale. sina li jan lawa pi kala en waso en soweli.\n",
      "tenpo sin la jan sewi Jawe li toki e ni: mi pana tawa sina e kasi ale en kiwen kasi ale. sina moku e ni.\n",
      "soweli ale en waso ale li moku e kasi.\n",
      "jan sewi li lukin. ale li pona mute.\n",
      "tenpo ni li tenpo suno nanpa luka wan.\n",
      "\n",
      "The first book of Moses.\n",
      "\n",
      "In the beginning, God created the blue sky and the big land. But the land was empty and everything was dark.\n",
      "The spirit of God moved upon the water.\n",
      "God said, \"Let there be light,\" and there was light.\n",
      "God saw the light, and it was good. He separated the light from the darkness.\n",
      "God called the light \"Day,\" and the darkness he called \"Night.\" This was the first day.\n",
      "\n",
      "God said, \"I am creating a large blue circle of water. It will not mix with the other waters.\"\n",
      "God created the circle of blue water. It was good.\n",
      "God called the circle of blue water \"Sky.\" This was the second day.\n",
      "\n",
      "God said, \"Let all the water come together in one place, and let the dry ground appear.\" It happened as he said.\n",
      "God called the dry ground \"Land,\" and the water he called \"Sea.\" God saw that it was good.\n",
      "God said, \"Let the land produce many plants.\" It happened as he said.\n",
      "This was the third day.\n",
      "\n",
      "God said, \"Let there be many stars in the sky. Let them separate the day from the night. Let them be signs to show the seasons, days, and years.\n",
      "Let them be lights in the sky to shine down on the earth.\" It happened as he said.\n",
      "God saw that it was good.\n",
      "This was the fourth day.\n",
      "\n",
      "God said, \"Let the water be filled with many living things, and let the air be filled with birds.\"\n",
      "God created many large fish, small fish, and birds. God saw that it was good.\n",
      "This was the fifth day.\n",
      "\n",
      "God said, \"Let there be many animals on the land.\" It happened as he said.\n",
      "God created many animals. God saw that it was good.\n",
      "\n",
      "God said, \"I will create human beings. They will be like me and rule over the fish, birds, and animals, and everything on the earth.\"\n",
      "God created male and female human beings. God blessed them and said to them, \"Have many children and fill the earth. Rule over the fish, birds, and animals.\"\n",
      "This was the sixth day\n"
     ]
    }
   ],
   "source": [
    "# Save the sentences and translations to a file.\n",
    "documents.to_csv(os.path.expanduser(\"documents.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "# Reload them and confirm that they are the same.\n",
    "documents_copy = pd.read_csv(os.path.expanduser(\"documents.tsv\"), sep='\\t', dtype={'id': 'int64', 'name': str, 'content_type': str, 'tok': str, 'eng': str, 'cmn': str if not pd.isnull('cmn') else None})\n",
    "pd.testing.assert_frame_equal(documents, documents_copy, check_dtype=True)\n",
    "\n",
    "\n",
    "# Find a story with an English translation\n",
    "story = documents[(documents['content_type'] == STORY) & (documents['eng'].notnull())].sample(1).iloc[0]\n",
    "print(story['name'])\n",
    "print(story['tok'])\n",
    "print(story['eng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toki Pona: 1183457 English: 88437 Chinese: 0\n"
     ]
    }
   ],
   "source": [
    "# For each document, find the word count and add it all up\n",
    "print('Toki Pona:', int(documents['tok'].str.split().str.len().sum()), 'English:', int(documents['eng'].str.split().str.len().sum()), 'Chinese:', documents['cmn'].str.split().str.len().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
